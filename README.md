<h1 align="center">SageLM<img src="Figs/logo-sage.png" alt="Â∞èÂõæÊ†á" width="35"/>: A Multi-aspect and Explainable Large Language Model for Speech Judgement</h1>
<!-- SageLM: A Multi-aspect and Explainable Large Language Model for Speech Judgement -->
<h4 align="center"> Yuan Ge, Junxiang Zhang, Xiaoqian Liu, Bei Li, Xiangnan Ma, Chenglong Wang, Kaiyang Ye, Yangfan Du, Linfeng Zhang, Yuxin Huang, Tong Xiao, Zhengtao Yu, Jingbo Zhu</h4>

<p align="center">
  üìÑ <a href="https://arxiv.org/abs/2508.20916">Paper</a> | 
  ü§ó <a href="https://huggingface.co/LGB666/SageLM">Model</a> | 
  ü§ó <a href="https://huggingface.co/datasets/LGB666/SageLM_testset_audio">Dataset</a>
</p>


## Newsüí°

- [2025.08] We release our paper. If you have any questions about our project, please send email to geyuanqaq@gmail.com
- Code, test dataset, and model will be released in a few days.
- The training dataset will be released after the paper is accepted.

## Quick Installation ‚öôÔ∏è

```
conda create -n sagelm python=3.10
conda activate sagelm
cd ./LLaMA-Factory
pip install -e .
```


## Usage üõ†

### Data Preparation

In order to use SageLM, you should first create a JSON file for your dataset in `./LLaMA-Factory/data`. Each entry should have the following format:

```
{
	"instruction": "...",  // prompt
	"input": "",  // leave empty
	"output": "...",  // label (used during training, leave empty during inference)
	"audios": [
		"",  // audio response 1
		""   // audio response 2
	]
}
```

We use the following prompt template for SageLM training and inference:

```
Below are two responses for a given task. The task is defined by the Instruction. Evaluate in terms of **{eval_dim}** and indicate a better response using 1, 2 or Tie.

### Instruction:
{question}

### Response 1:
<audio>

### Response 2:
<audio>

```

where `{eval_dim}` represents the evaluation dimension , `{question}` represents the user query, and \<audio\> serves as a placeholder for audio responses.

Next, register your dataset in `./LLaMA-Factory/data/dataset_info.json`. For example:

```
"test_semantic": {
    "file_name": "test_semantic.json",
    "columns": {
        "prompt": "instruction",
        "query": "input",
        "response": "output",
        "audios": "audios"
    }
}
```



We have released our test dataset at https://huggingface.co/LGB666/SageLM_testset_audio. After downloading, please move and rename the directory to `./Audio` and follow the steps above to register datasets.



### Model Inference

```
cd ./LLaMA-Factory
bash ./LLaMA-Factory/scripts/my_infer/infer.sh
```

Notice: SageLM currently supports only **English** and evaluated audio (audio 1 & audio 2) is **truncated to 60 s**.

We currently only support batch inference with JSON datasets, but inference can also be performed using the [Qwen2.5-Omni](https://github.com/QwenLM/Qwen2.5-Omni) official code.



### Evaluation

We released our evaluation scripts to reproduce the main results in our paper.

To evaluate the model's udging performance on **semantic** dimensions, run:

```
bash ./LLaMA-Factory/scripts/eval.sh
```

Note that each response pair in prediction and ground-truth files should be splited into four semantic dimensions, in the order of `helpfulness, honesty, instruction_following, truthfulness`. The data order should be consistent between the prediction and the ground-truth files.

To evaluate the model's judging performance on **acoustic** dimensions, run:

```
bash ./LLaMA-Factory/scripts/eval_stage2.sh
```

The acoustic evaluation should be performed on one of the following dimensions: `emotion instruction following, gender instruction following, character instruction following, gender instruction following and emotion instruction following`. The data order should also be consistent between the prediction and the ground-truth.



## Training of SageLM Model üìú

We trained our model using [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory). 

To train your own model, you need to register your dataset in `./LLaMA-Factory/data/dataset_info.json`. Then, specify the dataset and other training parameters in the YAML configuration file. We provide an example configuration file at `examples/judge/qwen2.5_omni_7B_compare_1_aspect.yaml`. Start training using the following commands:

```
cd ./LLaMA-Factory
llamafactory-cli train examples/judge/qwen2.5_omni_7B_compare_1_aspect.yaml
```

## Citation 
If you find our paper useful, please consider citing:
```bibtex
@article{ge2025sagelm,
  title={SageLM: A Multi-aspect and Explainable Large Language Model for Speech Judgement},
  author={Ge, Yuan and Zhang, Junxiang and Liu, Xiaoqian and Li, Bei and Ma, Xiangnan and Wang, Chenglong and Ye, Kaiyang and Du, Yangfan and Zhang, Linfeng and Huang, Yuxin and others},
  journal={arXiv preprint arXiv:2508.20916},
  year={2025}
}
```
